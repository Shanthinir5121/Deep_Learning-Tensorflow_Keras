{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset\n",
        "drive_dataset_path = '/content/drive/MyDrive/MNSIT_digits'\n",
        "\n",
        "# Copy dataset locally to speed up access\n",
        "print(\"Copying dataset to local storage (this makes it much faster)...\")\n",
        "!cp -r \"$drive_dataset_path\" /content/\n",
        "base_dir = '/content/MNSIT_digits'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'trainingSet')\n",
        "test_dir = os.path.join(base_dir, 'testSet')"
      ],
      "metadata": {
        "id": "fRRd_ggmY_oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177cd470-339c-4077-ddd1-9474ff96fa81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying dataset to local storage (this makes it much faster)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
        "import time\n",
        "\n",
        "base_dir = '/content/MNSIT_digits/trainingSet'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# LeNet-5\n",
        "lenet = Sequential([\n",
        "    Conv2D(6, (5,5), activation='relu', input_shape=(28,28,1), padding='same'),\n",
        "    AveragePooling2D(pool_size=(2,2)),\n",
        "    Conv2D(16, (5,5), activation='relu'),\n",
        "    AveragePooling2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(120, activation='relu'),\n",
        "    Dense(84, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "lenet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining LeNet-5...\")\n",
        "start = time.time()\n",
        "lenet.fit(train_generator, epochs=10, validation_data=val_generator, verbose=1)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "val_loss, val_acc = lenet.evaluate(val_generator, verbose=0)\n",
        "print(f\"\\nLeNet-5 Validation Accuracy: {val_acc*100:.2f}%  |  Time: {elapsed:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lBu-BnMhzIve",
        "outputId": "f3ff256e-c666-48ba-e495-ce18509440e1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 33611 images belonging to 10 classes.\n",
            "Found 8398 images belonging to 10 classes.\n",
            "\n",
            "Training LeNet-5...\n",
            "Epoch 1/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 68ms/step - accuracy: 0.7747 - loss: 0.7175 - val_accuracy: 0.9545 - val_loss: 0.1440\n",
            "Epoch 2/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.9620 - loss: 0.1227 - val_accuracy: 0.9709 - val_loss: 0.0948\n",
            "Epoch 3/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.9756 - loss: 0.0809 - val_accuracy: 0.9758 - val_loss: 0.0784\n",
            "Epoch 4/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 64ms/step - accuracy: 0.9804 - loss: 0.0563 - val_accuracy: 0.9739 - val_loss: 0.0812\n",
            "Epoch 5/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.9835 - loss: 0.0483 - val_accuracy: 0.9830 - val_loss: 0.0564\n",
            "Epoch 6/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.9869 - loss: 0.0397 - val_accuracy: 0.9789 - val_loss: 0.0699\n",
            "Epoch 7/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.9880 - loss: 0.0361 - val_accuracy: 0.9820 - val_loss: 0.0581\n",
            "Epoch 8/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.9903 - loss: 0.0307 - val_accuracy: 0.9834 - val_loss: 0.0531\n",
            "Epoch 9/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 65ms/step - accuracy: 0.9922 - loss: 0.0239 - val_accuracy: 0.9852 - val_loss: 0.0481\n",
            "Epoch 10/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9931 - loss: 0.0210 - val_accuracy: 0.9839 - val_loss: 0.0561\n",
            "\n",
            "LeNet-5 Validation Accuracy: 98.39%  |  Time: 346.95s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN model\n",
        "ann = Sequential([\n",
        "    Flatten(input_shape=(28,28,1)),          # Flatten 28x28 grayscale image\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "ann.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining ANN...\")\n",
        "start = time.time()\n",
        "ann.fit(train_generator, epochs=10, validation_data=val_generator, verbose=1)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "val_loss, val_acc = ann.evaluate(val_generator, verbose=0)\n",
        "print(f\"\\nANN Validation Accuracy: {val_acc*100:.2f}%  |  Time: {elapsed:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PsypC2Tpzamh",
        "outputId": "3ede00aa-89fa-41c7-be61-25e7702ab066"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ANN...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.8639 - loss: 0.4633 - val_accuracy: 0.9565 - val_loss: 0.1381\n",
            "Epoch 2/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9693 - loss: 0.1029 - val_accuracy: 0.9581 - val_loss: 0.1358\n",
            "Epoch 3/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.9775 - loss: 0.0711 - val_accuracy: 0.9731 - val_loss: 0.0912\n",
            "Epoch 4/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.9864 - loss: 0.0421 - val_accuracy: 0.9687 - val_loss: 0.0998\n",
            "Epoch 5/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.9897 - loss: 0.0325 - val_accuracy: 0.9726 - val_loss: 0.0908\n",
            "Epoch 6/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.9907 - loss: 0.0290 - val_accuracy: 0.9758 - val_loss: 0.0869\n",
            "Epoch 7/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9925 - loss: 0.0244 - val_accuracy: 0.9758 - val_loss: 0.0931\n",
            "Epoch 8/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.9930 - loss: 0.0192 - val_accuracy: 0.9749 - val_loss: 0.0981\n",
            "Epoch 9/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 39ms/step - accuracy: 0.9942 - loss: 0.0195 - val_accuracy: 0.9665 - val_loss: 0.1472\n",
            "Epoch 10/10\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.9918 - loss: 0.0248 - val_accuracy: 0.9770 - val_loss: 0.1026\n",
            "\n",
            "ANN Validation Accuracy: 97.70%  |  Time: 203.44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Model':<15}{'Accuracy (%)':<20}{'Training Time (s)':<20}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'ANN':<15}{ann_acc*100:<20.2f}{ann_time:<20.2f}\")\n",
        "print(f\"{'LeNet-5':<15}{lenet_acc*100:<20.2f}{lenet_time:<20.2f}\")\n",
        "print(\"-\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQiJ7Qb0477w",
        "outputId": "8994cd27-bb21-4435-e3ef-fd58cb003408"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL COMPARISON SUMMARY\n",
            "------------------------------------------------------------\n",
            "Model          Accuracy (%)        Training Time (s)   \n",
            "------------------------------------------------------------\n",
            "ANN            97.87               206.22              \n",
            "LeNet-5        98.54               346.64              \n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img_path = '/content/img_95.jpg'  # your test image path\n",
        "img = image.load_img(img_path, color_mode='grayscale', target_size=(28, 28))\n",
        "img_array = image.img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)  # make it (1,28,28,1)\n",
        "\n",
        "# Predict using both models\n",
        "ann_pred = np.argmax(ann.predict(img_array), axis=1)[0]\n",
        "lenet_pred = np.argmax(lenet.predict(img_array), axis=1)[0]\n",
        "\n",
        "print(f\"ANN Prediction: {ann_pred}\")\n",
        "print(f\"LeNet-5 Prediction: {lenet_pred}\")\n",
        "\n",
        "plt.imshow(img_array[0].reshape(28,28), cmap='gray')\n",
        "plt.title(f\"ANN: {ann_pred} | LeNet-5: {lenet_pred}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "G_WhZoW672MJ",
        "outputId": "ef54cbf4-974e-4425-d7ff-66300f4c0df3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "ANN Prediction: 1\n",
            "LeNet-5 Prediction: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ryImOFpa8OLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}